# k8s-webapp-depl

Задача заключается в развертывании веб-приложения в кластере Kubernetes со следующими требованиями:
Мультизональный кластер с 3 зонами и 5 узлами.
Инициализация приложения занимает 5–10 секунд.
4 пода достаточно для пиковой нагрузки.
Использование CPU высокое на старте (~1 CPU), затем стабилизируется на ~0.1 CPU. Память стабильно ~128 МиБ.
Суточный цикл нагрузки: низкая ночью, пиковая днем.
Обеспечить максимальную отказоустойчивость и минимальное потребление ресурсов.

Решение включает следующие ресурсы Kubernetes:
Depl: Управляет 4 подами по умолчанию с лимитами ресурсов, пробами готовности/запуска и ограничениями топологии для распределения по зонам.
HorizontalPodAutoscaler (HPA): Масштабирует поды от 2 (ночь) до 6 (пик) на основе загрузки CPU.
PodDisruptionBudget (PDB): Гарантирует доступность минимум 3 подов во время сбоев.
Service: Обеспечивает внутренний доступ к приложению через ClusterIP.

Арха:
Поды распределяются по зонам с помощью topologySpreadConstraints и по узлам с помощью podAntiAffinity для повышения отказоустойчивости.
HPA оптимизирует использование ресурсов, уменьшая количество подов ночью до 2.
PDB обеспечивает доступность во время обновлений или сбоев узлов.
Проба готовности и запуска гарантируют, что поды полностью инициализированы (5–10 секунд) перед приемом трафика.

Требования:
Кластер Kubernetes с 3 зонами и 5 узлами.
Утилита kubectl, настроенная для доступа к кластеру.
Включен Metrics Server для работы HPA на основе метрик CPU.


Небольшие уточнения:
В качестве заглушки используется образ nginx:1.14.2. В реальном приложении замените на образ с endpoint /health.
Предполагается, что кластер имеет метки зон (topology.kubernetes.io/zone) для работы topologySpreadConstraints.
HPA использует метрику CPU (цель — 70%). В продакшене можно настроить Prometheus для пользовательских метрик.
Манифесты проверены с помощью kubectl apply --dry-run=client.

Возможные улучшения:
Добавил бы ресурс Ingress для внешнего доступа.
Еще бы использовал ArgoCD или Flux для развертывания в стиле GitOps.
Ну и чего-нибудь для сложных метрик (если надо)

Обоснование решений:
4 пода по умолчанию: Основано на результатах нагрузочного теста, где 4 пода справляются с пиковой нагрузкой.
HPA (2–6 подов): Обеспечивает экономию ресурсов ночью (2 пода) с запасом для пиков (6 подов).
topologySpreadConstraints: Гарантирует распределение подов по зонам для защиты от сбоев зоны.
podAntiAffinity: Распределяет поды по узлам для отказоустойчивости на уровне узлов.
PDB (минимум 3 пода): Балансирует доступность и производительность во время сбоев.





Инструкция по развертыванию:
1. Клон репа
git clone https://github.com/BakhaDj/k8s-webapp-depl.git
cd k8s-webapp-deployment

2. Манифесты:
kubectl apply -f depl.yaml -> hpa.yaml -> pdb.yaml -> service.yaml

3. Чек статуса развертывания:
kubectl geet pods - o wide
kubectl get hpa -> pdb -> svc

4. Чек доступа к сервису внутри кластера:
kubectl port-forward svc/webapp-service 8080:80 (про nginx писал выше)

5. Проверим распределением подов по зонам:
kubectl get pods -o wide

6. Имитируем нагрузку (просто к примеру, hey -n 1000 -c 50 http://localhost:8080) и чек hpa:
kubectl get hpa --watch

7. Чек pdb, пробуем вывести узел из кластера:
kubectl drain <нод> --ignore-daemonsets

8. Чек проб, просмотр логов подов для чека работы проб запуска и готовности:
kubectl logs <имя>
