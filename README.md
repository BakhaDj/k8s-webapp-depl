# k8s-webapp-depl

Задача заключается в развертывании веб-приложения в кластере Kubernetes со следующими требованиями:
Мультизональный кластер с 3 зонами и 5 узлами.
Инициализация приложения занимает 5–10 секунд.
4 пода достаточно для пиковой нагрузки.
Использование CPU высокое на старте (~1 CPU), затем стабилизируется на ~0.1 CPU. Память стабильно ~128 МиБ.
Суточный цикл нагрузки: низкая ночью, пиковая днем.
Обеспечить максимальную отказоустойчивость и минимальное потребление ресурсов.

Решение включает следующие ресурсы Kubernetes:
Deployment: Управляет 4 подами по умолчанию с лимитами ресурсов, пробами готовности/запуска и ограничениями топологии для распределения по зонам.
HorizontalPodAutoscaler (HPA): Масштабирует поды от 2 (ночь) до 6 (пик) на основе загрузки CPU.
PodDisruptionBudget (PDB): Гарантирует доступность минимум 3 подов во время сбоев.
Service: Обеспечивает внутренний доступ к приложению через ClusterIP.

Арх-а:
Поды распределяются по зонам с помощью topologySpreadConstraints и по узлам с помощью podAntiAffinity для повышения отказоустойчивости.
HPA оптимизирует использование ресурсов, уменьшая количество подов ночью до 2.
PDB обеспечивает доступность во время обновлений или сбоев узлов.
Проба готовности и запуска гарантируют, что поды полностью инициализированы (5–10 секунд) перед приемом трафика.

Требования:
Кластер Kubernetes с 3 зонами и 5 узлами.
Утилита kubectl, настроенная для доступа к кластеру.
Включен Metrics Server для работы HPA на основе метрик CPU.


Небольшие уточнения:
В качестве заглушки используется образ nginx:1.14.2. В реальном приложении замените на образ с endpoint /health.
Предполагается, что кластер имеет метки зон (topology.kubernetes.io/zone) для работы topologySpreadConstraints.
HPA использует метрику CPU (цель — 70%). В продакшене можно настроить Prometheus для пользовательских метрик.
Тестирование не проводилось из-за отсутствия кластера (если применимо). Манифесты проверены с помощью kubectl apply --dry-run=client.

Возможные улучшения:
Добавить ресурс Ingress для внешнего доступа.
Интегрировать Prometheus и Grafana для мониторинга и алертов.
Использовать ArgoCD или Flux для развертывания в стиле GitOps.
Настроить KEDA для масштабирования на основе сложных метрик.

Обоснование решений:
4 пода по умолчанию: Основано на результатах нагрузочного теста, где 4 пода справляются с пиковой нагрузкой.
HPA (2–6 подов): Обеспечивает экономию ресурсов ночью (2 пода) с запасом для пиков (6 подов).
topologySpreadConstraints: Гарантирует распределение подов по зонам для защиты от сбоев зоны.
podAntiAffinity: Распределяет поды по узлам для отказоустойчивости на уровне узлов.
PDB (минимум 3 пода): Балансирует доступность и производительность во время сбоев.

